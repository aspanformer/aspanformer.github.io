<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ASpanFormer utilizes global-local attention sturcture to pass messages across views for robust and accurate image matching, specially an
        uncertainty driven framework is used to adaptivelty adust local attention span.">
  <meta name="keywords" content="Image Matching, Transformer, Adaptive Attention Span">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ASpanFormer: Detector-Free Image Matching
            with Adaptive Span Transformer</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Hongkai Chen,</span>
            <span class="author-block">
              <a href="https://lzx551402.github.io/">Zixin Luo</a>,</span>
            <span class="author-block">
              <a href="https://zlthinker.github.io/">Lei Zhou</a>,</span>
            <span class="author-block">
              Yurun Tian,
            </span>
            <span class="author-block">
              <a href="https://mingminzhen.github.io/">Mingmin Zhen</a>,
            </span>
            <span class="author-block">
              <a>Tian Fang</a>,
            </span>   <br> 
            <span class="author-block">
              David McKinnon,
            </span>
            <span class="author-block">
              Yanghai Tsin,
            </span>
            <span class="author-block">
              <a href="https://www.cse.ust.hk/~quan/">Long Quan</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Hong Kong University of Science and Technology,</span>
            <span class="author-block"><sup>2</sup>Apple Inc.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(coming soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center><img src="static/images/teaser.png" alt="Alternative Text" class="center"></center>
      <h2 class="subtitle has-text-centered">
        ASpanFormer adaptively captures necessary context according
        to matching difficulty.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Generating robust and reliable correspondences across images is a fundamental task for a diversity of applications. To capture context at both global and local granularity, we propose ASpanFormer, a Transformer-based detector-free matcher that is built on hierarchical attention structure, adopting a novel attention operation which is capable of adjusting attention span in a self-adaptive manner. To achieve this goal, first, flow maps are regressed in each cross attention phase to locate the center of search region. Next, a sampling grid is generated around the center, whose size, instead of being empirically configured as fixed, is adaptively computed from a pixel uncertainty estimated along with the flow map. Finally, attention is computed across two images within derived regions, referred to as attention span. By these means, we are able to not only maintain long-range dependencies, but also enable fine-grained attention among pixels of high relevance that compensates essential locality and piece-wise smoothness in matching tasks. State-of-the-art accuracy on a wide range of evaluation benchmarks validates the strong matching capability of our method.
          </p>
        </div>
      </div>
    </div>
  </div>
    <!--/ Abstract. -->

    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Method</h2>
          <p> CNN extracts initial features. After initialization, the features are fed into iterative Global-Local Attention (GLA) blocks for updating. A matching module is used to
            determine final matches
          </p>
          <center><img src="static/images/network.png" alt="Alternative Text" width="80%" class="center"></center>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  
    <!-- Visualization. -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visualization</h2>
          <center><img src="static/images/matching.png" alt="Alternative Text" width="80%" class="center"></center>
        </div>
      </div>
    </div>
    <!--/ Visualization. -->

    <!-- Benchmarking. -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Benchmarking</h2>
          <center><img src="static/images/res.png" alt="Alternative Text" width="80%" class="center"></center>
        </div>
      </div>
    </div>
    <!--/ Benchmarking. -->

  </div>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2022aspanformer,
  author    = {Chen, Hongkai and Luo, Zixin and Zhou, Lei and Tian, Yurun and Zhen, Mingmin and Fang, Tian and McKinnon, David and Tsin, Yanghai and Quan, Long},
  title     = {ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer},
  journal   = {ECCV},
  year      = {2022},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/vdvchen" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
